üßôüèæ‚Äç‚ôÇÔ∏è **AI\_COR (Agent Orchestrator)**
: I am an expert in agentic coding systems. I know Claude Code, your Context Engineering (CE) workflow, BMAD, Archon-style context packs, and SAGE (Self-Assessment ‚Üí Adaptive Planning ‚Üí Goal-oriented Execution ‚Üí Experience Integration). I will reason step-by-step to define (1) the high-level components and (2) a concrete end-to-end workflow example. I can use test gating, reflexion points, and persistent experience memory. I will help by: specifying responsibilities, inputs/outputs, and run-time hand-offs between modules, then illustrating it on a realistic feature change. My task ends when you have a clear system diagram, interfaces, and an example trace you can implement.

---

# 1) Major high-level components

## A. Orchestrator (SAGE Controller)

* **Role:** Runs the control loop: Self-Assessment ‚Üí Adaptive Planning ‚Üí Execute ‚Üí Reflexion ‚Üí Experience Integration.
* **Inputs:** Goal, repo context, CE test matrix, policy/config.
* **Outputs:** `plan.md`, `success_criteria.md`, step tickets, reflexion notes, memory entries.

## B. Adaptive Planner

* **Role:** Converts the goal into a *live* plan (revised mid-run). Splits into steps, defines checkpoints, chooses tools.
* **Inputs:** Goal profile, repo scan, CE tests, prior memory.
* **Outputs:** Ordered steps with acceptance checks, tool/command templates.

## C. Goal-Oriented Executor (Claude Code Adapter)

* **Role:** Turns a *step* into safe code edits/commands via Claude Code; requests diffs/patches, runs, and PRs.
* **Inputs:** Step spec, guardrail policies, test commands.
* **Outputs:** Diffs, run logs, PR branches, pass/fail signals.

## D. Guardrail Layer (Safety & Dry-Run)

* **Role:** Enforces **non-destructive by default**: dry-run previews, ‚Äúdangerous op‚Äù detection, branch/backup, migration checks.
* **Inputs:** Intended changes/commands.
* **Outputs:** Abort/allow decisions; red flags recorded to memory.

## E. CE Test Harness (Monitor in BMAD)

* **Role:** Executes **PRP groups** (Happy ‚Üí Edge ‚Üí Negative); produces machine-readable verdicts.
* **Inputs:** Test selection for current step/feature.
* **Outputs:** Structured results (+ artifacts): pass/fail, stack traces, coverage deltas.

## F. Reflexion Engine (Critic)

* **Role:** Adds critique at **three points**: step-level, phase checkpoint, end-of-run. Suggests smallest viable fix; proposes plan edits.
* **Inputs:** Diffs, logs, failures, criteria.
* **Outputs:** ‚ÄúWhy it failed‚Äù, ‚ÄúNext attempt‚Äù, plan patches, heuristics to persist.

## G. Experience Memory (Global & Local)

* **Role:** Writes/reads durable lessons.
* **Structure:**

  * `memory/global.jsonl` (portable patterns, e.g., ‚Äúfor Django apps, verify INSTALLED\_APPS before running tests‚Äù),
  * `memory/local.jsonl` (repo-specific),
  * `memory/tools.jsonl` (tool timings, flakiness).
* **Usage:** Loaded on start; appended on end-of-run.

## H. Archon-Style Context Pack

* **Role:** Reproducible, minimal **.agent/** package that stores plan, criteria, reflexion, memory, config; supports segment resets.
* **Key files:**

  * `.agent/plan.md`, `.agent/success_criteria.md`, `.agent/reflexion.md`, `.agent/checkpoints/*`, `.agent/settings.json`.

## I. Tooling & Environment Manager

* **Role:** Encapsulates shell/CLI, package managers, DB/migrations, docker compose, seed data.
* **Outputs:** Deterministic run envelopes (same commands, same environment).

## J. Observability & Run Log

* **Role:** Timestamped trace of steps, prompts, diffs, test outcomes, and decisions; aids rollbacks and audits.

## K. Policy & Config

* **Role:** Defines destructive-ops policy, branch/PR naming, timeouts, retry budgets, and which CE test groups are gates at each checkpoint.

---

# 2) What a workflow looks like (concrete example)

> **Scenario:** Add a ‚ÄúCSV export‚Äù endpoint to a Django app‚Äôs `/reports` module.
> **Goal:** ‚ÄúAs an analyst, I can download a CSV of the current report with columns A,B,C under role-based access.‚Äù
> **Repo:** Python/Django + pytest + ruff + mypy.
> **Acceptance (CE Success Criteria):**
>
> 1. GET `/api/reports/export?filters=‚Ä¶` returns 200 with CSV header `A,B,C`.
> 2. Admin and Analyst roles only.
> 3. Row count equals filtered query.
> 4. Tests: `tests/reports/test_export.py::test_happy_csv`, `::test_auth`, `::test_filtering` pass.
> 5. Lint/type checks pass.
> 6. No migration warnings or secret leakage.

---

## Phase 0 ‚Äî Self-Assessment (SAGE)

* **Orchestrator** creates `.agent/success_criteria.md` from the goal + above checks.
* Scans repo for `reports/`, DRF views/routers, auth model, and existing CSV utils.
* **Risks:** paging vs whole download; large query memory; permission mixin placement.
* **Tools:** pytest, ruff, mypy, Django shell; CSV stdlib.

**Artifacts:**

* `.agent/checkpoints/2025-08-16_goal.json` (goal, criteria, risks, tool list)

---

## Phase 1 ‚Äî Adaptive Planning (SAGE + BMAD ‚ÄúBreak‚Äù)

Planner writes `.agent/plan.md`:

**Plan v1 (excerpt):**

1. **Design**: record API contract + auth model; add CE test stubs if missing.
2. **Implement**: DRF route `GET /api/reports/export` with streaming CSV, role guard.
3. **Wire filters**: reuse existing query builder; verify count.
4. **Tests**: happy path, auth denied, filter correctness, large dataset smoke.
5. **Docs**: update README and OpenAPI.

* Each step has **checkpoint**: which tests/linters must pass before moving on.

---

## Phase 2 ‚Äî Goal-Oriented Execution (BMAD ‚ÄúMonitor/Adjust‚Äù inside)

For **each step**:

### Step 1 (Design)

* **Guardrail (dry-run):** Show intended file additions: `reports/export.py`, `urls.py` patch, tests added.
* **Claude Code (executor):** Generates tests first (TDD bias) in `tests/reports/test_export.py`.
* **CE Harness:** run `pytest -q tests/reports/test_export.py::test_happy_csv -q` ‚Üí **fails** (no route yet).

**Reflexion (step-level):**

* Critic notes: ‚ÄúMissing route and view; prefer `StreamingHttpResponse` to avoid memory blow-ups; add role check.‚Äù
* **Replan (micro):** Insert ‚Äúadd route + minimal view‚Äù before full streaming.

### Step 2 (Implement minimal route + view)

* **Guardrail:** Print diff; ensure no model migrations.
* **Executor:** Adds DRF view, returns stub CSV header.
* **Tests:** happy now **passes** header; auth & filtering still failing.

**Reflexion:**

* ‚ÄúAdd role guard via permission classes; wire filter parser.‚Äù

### Step 3 (Auth + Filters)

* **Executor:** Adds `IsAdminOrAnalyst` permission; integrates existing query builder.
* **Tests:** `test_auth` passes; `test_filtering` fails on off-by-one.

**Reflexion:**

* ‚ÄúEdge case in date range; adjust end-inclusive logic; add edge test.‚Äù

* **Re-execute:** Filtering now passes.

### Step 4 (Performance & streaming)

* **Guardrail:** Large dataset smoke test behind feature flag; dry-run prints memory usage estimate.
* **Executor:** Switches to `StreamingHttpResponse` generator; chunking size added.
* **Tests:** pass; lint/type pass.

### Step 5 (Docs/OpenAPI)

* **Executor:** Updates OpenAPI schema; adds README section.
* **Tests:** none required; run full suite to be safe.

---

## Phase 3 ‚Äî Deliver (BMAD ‚ÄúDeliver‚Äù)

* **PR Creation:** Executor opens PR `feat/reports-csv-export` with checklist: criteria ‚úÖ, test summary, diff, and risk notes.
* **Observability:** Run log saved; tag created.
* **Orchestrator:** declares success.

---

## Phase 4 ‚Äî Experience Integration (SAGE)

Append lessons to memory:

`memory/global.jsonl`

```json
{"when":"2025-08-16T20:04Z","topic":"Django CSV export","pattern":"prefer StreamingHttpResponse for large exports","guard":"add dataset size smoke gate","tests":["test_happy_csv","test_filtering","test_auth"],"confidence":0.85}
```

`memory/local.jsonl`

```json
{"project":"acme-reports","endpoint":"/api/reports/export","filter_engine":"reports.filters.QueryBuilder","edge_case":"date range end-inclusive bug","fix":"normalize end date","confidence":0.9}
```

`memory/tools.jsonl`

```json
{"tool":"pytest subset run","note":"fast feedback; keep per-step targets small"}
```

**Effect now/future:**

* *Now:* You already benefited from **step-level reflexion** (replanning within the run).
* *Future:* New repos or features load `global.jsonl` and pre-apply the streaming + edge tests; this reduces rediscovery time.

---

## Swimlane view (who does what)

* **Orchestrator:** owns loop, writes plan/criteria, decides replan vs continue.
* **Planner:** edits `plan.md` when reflexion suggests better ordering.
* **Executor (Claude Code):** proposes patches, shows diffs, runs tests, opens PR.
* **Guardrail:** blocks unsafe ops; enforces branch discipline and backups.
* **CE Harness:** is the ‚Äútruth oracle‚Äù for pass/fail.
* **Reflexion Engine:** critiques and proposes smallest viable fixes.
* **Experience Memory:** persists what we learned for reuse.

---

## What changes if something breaks badly?

* A large failure (e.g., auth model mismatch) triggers a **phase-checkpoint reflexion**: the Planner may **replace** Step 2 with a smaller spike, or add a migration safety net. The Orchestrator records a new plan version in `.agent/checkpoints/`.

---

### Optional mini-example: Frontend tweak

**Goal:** Add a ‚ÄúDownload CSV‚Äù button in React, using the new endpoint.

* Same loop applies; tests = unit + e2e.
* Guardrail blocks committing if the button appears for unauthorized roles.
* Reflexion captures a UI rule: ‚Äúdisable action if `403` or role mismatch; show toast.‚Äù

---

## Implementation starter (files you‚Äôll want)

```
.agent/
  plan.md
  success_criteria.md
  reflexion.md
  checkpoints/
  settings.json
  memory/{global.jsonl,local.jsonl,tools.jsonl}
scripts/
  guard_dry_run.sh
  guard_safety.sh
  run_reflexion.py
```

---

Would you like me to **generate this starter kit** (files + skeleton content + prompt stubs) tailored to one of your active repos, or map it directly onto your CE `/execute-prp` flow?
